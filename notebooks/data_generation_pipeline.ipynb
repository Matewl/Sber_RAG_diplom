{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from IPython.display import Markdown, display\n",
    "from typing import List\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "import langchain_gigachat\n",
    "from langchain_gigachat import GigaChat, GigaChatEmbeddings\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    " \n",
    "def prepare_embs(emb):\n",
    "    emb = np.array(emb)\n",
    "    emb /= (((emb**2).sum(1))**0.5).reshape((emb.shape[0], 1))\n",
    "    emb = emb.tolist()\n",
    "    return emb\n",
    "\n",
    "def prepare_emb(emb):\n",
    "    emb = np.array(emb)\n",
    "    emb /= ((emb**2).sum())**0.5\n",
    "    emb = emb.tolist()\n",
    "    return emb  \n",
    "\n",
    "def display_markdown(text):\n",
    "    display(Markdown(text.replace('Модели оценки кредитного риска', '')))\n",
    "\n",
    "from langchain_gigachat import GigaChat, GigaChatEmbeddings\n",
    "\n",
    "llm = GigaChat(\n",
    "            model= 'GigaChat-Pro',\n",
    "    )\n",
    "\n",
    "\n",
    "llm_max = GigaChat(\n",
    "            model = 'GigaChat-2-Max',\n",
    ")\n",
    "\n",
    "\n",
    "llm_max_strict = GigaChat(\n",
    "            model = 'GigaChat-2-Max',\n",
    ")\n",
    "\n",
    "\n",
    "llm_deepseek_strict = GigaChat(\n",
    "            model = 'DeepSeek-R1',\n",
    ")\n",
    "\n",
    "\n",
    "llm_deepseek = GigaChat(\n",
    "            model = 'DeepSeek-R1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Читаем уже созданные чанки и вопросы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(data_path,\n",
    "                             None,\n",
    "                             allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_keys = list(docs.keys())\n",
    "docs[docs_keys[0]].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_texts = [docs[key].page_content for key in docs_keys]\n",
    "docs_metas = [docs[key].metadata for key in docs_keys]\n",
    "len(docs_texts), len(set(docs_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = pd.read_excel('Прошлые вопросы (RAG).xlsx')\n",
    "queries = pd.read_excel('C:\\\\Users\\\\22589910\\\\Desktop\\\\diplom\\\\indexes\\\\Документы + вопросы с ответами. БЕНЧИ КУЧКА 4.xlsx')\n",
    "print(len(queries))\n",
    "queries = queries.dropna()\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in queries['Вопрос ']:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем простые вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GEN_QUESTIONS_SYSTEM_PROMPT = \"\"\"NDA\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_RELEVANCE_CHUNK = \"\"\"NDA\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ContextEstimation(BaseModel):\n",
    "\"\"\"NDA\"\"\"\n",
    "    \n",
    "class QuestionGenerationItem(BaseModel):\n",
    "\"\"\"NDA\"\"\"\n",
    "\n",
    "class MultipleQuestionGeneration(BaseModel):\n",
    "\"\"\"NDA\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_relevance_chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', CHECK_RELEVANCE_CHUNK),\n",
    "    ('user', 'Отрывок, который необходимо оценить:\\n{text}')\n",
    "])\n",
    "\n",
    "gen_questions_chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', GEN_QUESTIONS_SYSTEM_PROMPT),\n",
    "    ('user', 'Отрывок, по которому необходимо составить вопросы:\\n{text}')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_questions_chat_template.invoke({'text': 'Привт', 'sample_questions': 'ddd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_relevance_pipe_deepseek = check_relevance_chat_template | llm_deepseek_strict | StrOutputParser()\n",
    "# gen_questions_pipe = gen_questions_chat_template | llm_deepseek | StrOutputParser()\n",
    "\n",
    "# check_relevance_pipe_deepseek = check_relevance_chat_template | llm_deepseek_strict.with_structured_output(ContextEstimation)\n",
    "# gen_questions_pipe_deepseek = gen_questions_chat_template | llm_deepseek.with_structured_output(MultipleQuestionGeneration)\n",
    "\n",
    "check_relevance_pipe = check_relevance_chat_template | llm_max_strict.with_structured_output(ContextEstimation)\n",
    "gen_questions_pipe = gen_questions_chat_template | llm_max.with_structured_output(MultipleQuestionGeneration)\n",
    "\n",
    "# check_relevance_pipe = check_relevance_chat_template | llm_max_strict | StrOutputParser()\n",
    "# gen_questions_pipe = gen_questions_chat_template | llm_max | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_questions(text):\n",
    "#     while True:\n",
    "#         try:\n",
    "#             is_relevant = check_relevance_pipe.invoke({'text': text})\n",
    "#             try:\n",
    "#                 is_relevant = json.loads(is_relevant)\n",
    "#             except:\n",
    "#                 print('bad_1')\n",
    "#                 return\n",
    "            \n",
    "#             if is_relevant['response'] == 'Нет' or is_relevant['response'] == 'нет':\n",
    "#                 return\n",
    "#             sample_questions = '\\n'.join(queries.sample(5)['Вопрос '].tolist())\n",
    "#             new_question = gen_questions_pipe.invoke({'sample_questions':sample_questions, 'text':text})\n",
    "\n",
    "#             try:\n",
    "#                 return text, json.loads(new_question)\n",
    "#             except:\n",
    "#                 print('bad_2')\n",
    "#                 return\n",
    "\n",
    "#         except:\n",
    "#             time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text):\n",
    "    while True:\n",
    "        try:\n",
    "            is_relevant = check_relevance_pipe.invoke({'text': text})\n",
    "            if is_relevant.response.strip(' \\n.').lower() == 'нет':\n",
    "            # if is_relevant.split('').split('</think>')[1].strip(' \\n.').lower()  == 'нет':\n",
    "                return None, None, None\n",
    "            sample_questions = '\\n'.join(queries.sample(5)['Вопрос '].tolist())\n",
    "            new_questions = gen_questions_pipe.invoke({'sample_questions': sample_questions, 'text': text})\n",
    "            questions = []\n",
    "            brief_questions = []\n",
    "            for question in new_questions.questions:\n",
    "                questions.append(question.question)\n",
    "                brief_questions.append(question.brief_question)\n",
    "        \n",
    "            return text, questions, brief_questions\n",
    "        except: \n",
    "            print('bad')\n",
    "            time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_questions(docs_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "result_2 = Parallel(n_jobs=4, prefer=\"threads\")(delayed(generate_questions)(text) for text in tqdm(docs_texts[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(result_2, open('res_1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "result_2 = Parallel(n_jobs=4, prefer=\"threads\")(delayed(generate_questions)(text) for text in tqdm(docs_texts[500:1000]))\n",
    "pickle.dump(result_2, open('res_2.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = Parallel(n_jobs=4, prefer=\"threads\")(delayed(generate_questions)(text) for text in tqdm(docs_texts[1000:1500]))\n",
    "pickle.dump(result_2, open('res_3.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = Parallel(n_jobs=4, prefer=\"threads\")(delayed(generate_questions)(text) for text in tqdm(docs_texts[1500:]))\n",
    "pickle.dump(result_2, open('res_4.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = docs_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_relevance_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_relevant = check_relevance_pipe.invoke({'text': text})\n",
    "\n",
    "# # if is_relevant.response == 'Нет' or is_relevant.response == 'нет':\n",
    "# #     1\n",
    "# # else:\n",
    "# sample_questions = '\\n'.join(queries.sample(5)['Вопрос '].tolist())\n",
    "# new_question = gen_questions_pipe.invoke({'sample_questions':sample_questions, 'text':text})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1337)\n",
    "# questions_docs = []\n",
    "# for text in tqdm(docs_texts):\n",
    "#     while True:\n",
    "#         try:\n",
    "#             is_relevant = check_relevance_pipe.invoke({'text': text})\n",
    "#             try:\n",
    "#                 is_relevant = json.loads(is_relevant)\n",
    "#             except:\n",
    "#                 break\n",
    "            \n",
    "#             if is_relevant['response'] == 'Нет' or is_relevant['response'] == 'нет':\n",
    "#                 break\n",
    "#             sample_questions = '\\n'.join(queries.sample(5)['Вопрос '].tolist())\n",
    "#             new_question = gen_questions_pipe.invoke({'sample_questions':sample_questions, 'text':text})\n",
    "\n",
    "#             try:\n",
    "#                 questions_docs.append((text, json.loads(new_question)))\n",
    "#             except:\n",
    "#                 break\n",
    "\n",
    "#             break\n",
    "#         except:\n",
    "#             time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получаем ответы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(1, 5):\n",
    "    res += pickle.load(open(f'res_{i}.pkl', 'rb'))\n",
    "\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [r for r in res if r[0] and len(r[1]) == 3 and len(r[2]) == 3]\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_question = []\n",
    "for r in res:\n",
    "    for i in range(3):\n",
    "        context_question.append((r[0], r[1][i]))\n",
    "        context_question.append((r[0], r[2][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUFF_SYSTEM = \"\"\"NDA\"\"\"\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', STUFF_SYSTEM),\n",
    "    ('user', '\"\"\"NDA\"\"\"')\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    chat_template | \n",
    "    llm_max_strict | \n",
    "    StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(doc, question):\n",
    "    while True:\n",
    "        try:    \n",
    "            answer = chain.invoke({'context': doc, 'question': question})            \n",
    "            return answer, doc, question\n",
    "        except:\n",
    "            print('bad')\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Parallel(n_jobs=4, prefer=\"threads\")(delayed(generate_answer)(doc, question) for doc, question in tqdm(context_question[2000:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "pickle.dump(result, open('answers_2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(118):\n",
    "#     question = json.loads(questions_docs[i][1])['question']\n",
    "#     doc = questions_docs[i][0]\n",
    "#     answer = answers[i]\n",
    "#     print('Question:')\n",
    "#     print(question)\n",
    "#     print()\n",
    "#     print('Context:')\n",
    "#     display_markdown(doc)\n",
    "#     print('Answer:')\n",
    "#     print(answer)\n",
    "#     print()\n",
    "#     print('-' * 150)\n",
    "#     print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
